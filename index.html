<html><head>
	<title>Pengwei Liang's Homepage</title>
	<style type="text/css">
	body {
		margin-top: 30px;
		margin-bottom: 30px;
		margin-left: 100px;
		margin-right: 100px;
	}
	p {
		margin-top: 0px;
		margin-bottom: 0px;
	}
	
	.caption {
		font-size: 34px;
		font-weight: normal;
		color: #000;
		font-family: Constantia, "Lucida Bright", "DejaVu Serif", Georgia, serif;
	}
	.caption-1 {
		font-size: 16px;
		font-family: Tahoma, Geneva, sans-serif;
	}
	.caption-2 {
		font-size: 16px;
		font-family: Tahoma, Geneva, sans-serif;
		font-weight: bold;
		color: #234A8C;
	}
	.caption-3 {
		font-size: 16px;
		font-family: Tahoma, Geneva, sans-serif;
		font-weight: bold;
		color: #F00;
	}
	
	.caption-4 {
		font-size: 16px;
		font-family: Tahoma, Geneva, sans-serif;
		color: #234A8C;
	}
	.content {
		font-size: 16px;
		font-family: Tahoma, Geneva, sans-serif;
		text-align: justify;
	}
	.content a {
		font-size: 16px;
		font-family: Tahoma, Geneva, sans-serif;
		color: #000;
	}
	.content strong a {
		font-size: 16px;
		font-family: Tahoma, Geneva, sans-serif;
		color: #234A8C;
	}
	.title-small {
		font-size: 20px;
		font-family: Georgia, "Times New Roman", Times, serif;
		font-weight: bold;
		color: #F90;
	}
	.title-large {
		font-size: 28px;
		font-family: Georgia, "Times New Roman", Times, serif;
		font-weight: bold;
		color: #000;
	}
	.margin {
		font-size: 10px;
		line-height: 10px;
	}
	.margin-small {
		font-size: 5px;
		line-height: 5px;
	}
	.margin-large {
		font-size: 16px;
		line-height: 16px;
	}
	a:link {
		text-decoration: none;
	}
	a:visited {
		text-decoration: none;
	}
	content a:link {
		text-decoration: none;
	}
	content a:visited {
		text-decoration: none;
	}
	a:hover {
		text-decoration: underline;
	}
	a:active {
		text-decoration: underline;
		color: #000000;
		font-family: Tahoma, Geneva, sans-serif;
	}
	strong a:active {
		text-decoration: underline;
		color: #000000;
	}
	img
	{
	 border-color: black;
	}
	
	
	</style>
	<meta http-equiv="Content-Type" content="text/html; charset=gbk">
	</head>
	
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
	
	  ga('create', 'UA-53682931-1', 'auto');
	  ga('send', 'pageview');
	
	</script>
	
	<body>
	
	<table border="0" width="100%">
	  <tbody>
	
		<tr>
	
		<td width="300"><table>
				<tr><td>
				<a href="imgs/me.jpg"><img src="imgs/me.jpg"  height="270" alt="Pengwei Liang; Hong Kong" border=1></td></tr>
				</table></td>
		<td width="15"></td>
		<td></td>
		<td><table border="0" width="100%">
		  <tbody><tr height="20">
			<td colspan="2"></td></tr>
	   <tr height="60">
			<td>
				   <p class="caption">Pengwei Liang (梁鹏伟)<br><br></p>
				   <p class="content">Ph.D.</p>
				<p class="content">Computer Science and Technology</p>
				  <p class="content">School of Computer Science and Technology</p>
				<p class="content">Harbin Institute of Technology</p><br>
				<p class="content"><strong>Email</strong>: erfect2020@gmail.com</p>
			</td>
		  </tr>
	
	
	
		  <tr height="40">
			<td>
			  <p class="margin">&nbsp;</p>
			  <p class="content"><strong>
				  <a href="papers/cszx_cv.pdf">CV</a></strong>  |
				  <strong><a href="https://scholar.google.com/citations?user=54Ci0_0AAAAJ">Google Scholar</a></strong> |
				  <strong><a href="https://github.com/erfect2020/">GitHub</a></strong> |
				  <strong><a href="#sect-publications">Papers</a></strong> |
				  <!-- <strong><a href="./papers/research_statement.pdf">Research Statement</a></strong> | -->
				  <!-- <strong><a href="pdf/thesis_highres.pdf">Thesis</a></strong> | -->
				  <!-- <strong><a href="#sect-teaching">Teaching</a></strong></p> -->
			<!-- <p class="content"><strong>
				<a href="#sect-software">Software</a></strong> |
				<strong><a href="#sect-publications">Papers</a></strong> |
				<strong><a href="#sect-talks">Talks</a></strong> |
				<strong><a href="#sect-awards">Awards</a></strong> |
				<strong><a href="https://arxiv.org/a/zhu_j_5.html">Arxiv</a></strong>
			</p> -->
			</td>
		  </tr>
		  <tr height="20">
			<td colspan="2"></td></tr>
		</tbody></table></td>
	  </tr>
	</tbody></table>
	<p class="margin">&nbsp;</p>
	
	<table border="0" >
	  <tbody>
		<tr>
		  <td width="900"> <p align="justify" class="content">
			  I received my Ph.D. from Harbin Institute of Technology, advised by <strong><a href="https://homepage.hit.edu.cn/jiangjunjun" target="_blank" rel="nofollow" class="caption-2">Prof. Junjun Jiang</a></strong> and <strong><a href="https://sites.google.com/site/jiayima2013/jiayi-ma-%E9%A9%AC%E4%BD%B3%E4%B9%89-professor" target="_blank" rel="nofollow" class="caption-2">Prof. Jiayi Ma</a></strong>. I obtained my M.S. from Wuhan University. My research focuses on image generation (AIGC), image restoration, and image editing with LLMs. These areas have broad applications, including generative AI and low-level visual enhancement.</p>
		  </tr>
	  </tbody>
	</table>
	
	<br>
	<br>
	
	
	<p id="sect-publications" class="title-large">Publications</p>
	<table border="0">
	<tbody><tr>
		<td width="140"><a href="http://erfect2020.github.io/"><img src="imgs/FusionGAN.jpg" border="1"width="210"></a></td>
		<td width="20"></td>
		<td valign="middle" width="900"><p class="content"><strong>FusionGAN: A generative adversarial network for infrared and visible image fusion.</strong></p>
		<p class="content">
			<a href="https://sites.google.com/site/jiayima2013/jiayi-ma-%E9%A9%AC%E4%BD%B3%E4%B9%89-professor">Jiayi Ma</a>,
			<a href="https://scholar.google.com/citations?user=VylFolAAAAAJ"> Wei Yu</a>,
			<a href="http://erfect2020.github.io/"><b>Pengwei Liang</b></a>, 
			<a href="https://scholar.google.com/citations?user=Fq0h6mcAAAAJ">Chang Li</a>, 
			<a href="http://homepage.hit.edu.cn/jiangjunjun">Junjun Jiang</a>
		</p>
		<p class="content">Information Fusion, 2019 (citations:2122)</p>
			<p class="margin-small">&nbsp;</p>
		<p class="content">
		<strong><a href="https://github.com/jiayi-ma/FusionGAN">Project</a></strong> |
				<strong> <a href="https://github.com/jiayi-ma/FusionGAN">Code</a></strong> |
				<strong><a href="https://www.sciencedirect.com/science/article/pii/S1566253518301143
					">Paper</a></strong> |
			    <strong><a href="bib/FusionGAN.bib" download>BibTeX</a></strong>
				<br> </p>  </tr>
	</tbody>
	</table>
	<table border="0">
	<tbody><tr>
		<td width="140"><a href="https://erfect2020.github.io/"><img src="imgs/BaMBNet.jpg" border="1"width="210"></a></td>
		<td width="20"></td>
		<td valign="middle" width="900"><p class="content"><strong>BaMBNet: A blur-aware multi-branch network for dual-pixel defocus deblurring.</strong></p>
		<p class="content"><a href="http://erfect2020.github.io/"><b>Pengwei Liang</b></a>,  <a href="http://homepage.hit.edu.cn/jiangjunjun">Junjun Jiang*</a>,
			<a href="http://homepage.hit.edu.cn/xmliu">Xianming Liu</a>,
			<a href="https://sites.google.com/site/jiayima2013/jiayi-ma-%E9%A9%AC%E4%BD%B3%E4%B9%89-professor">Jiayi Ma</a>
		</p>
		<p class="content">IEEE/CAA Journal of Automatica Sinica (JAS), 2022</p>
			<p class="margin-small">&nbsp;</p>
		<p class="content">
		<strong><a href="https://github.com/junjun-jiang/BaMBNet">Project</a></strong> |
				<strong> <a href="https://github.com/junjun-jiang/BaMBNet">Code</a></strong> |
				<strong><a href="https://www.ieee-jas.net/en/article/doi/10.1109/JAS.2022.105563">Paper</a></strong> |
				<strong><a href="bib/BaMbNet.bib" download>BibTeX</a></strong>
			<br> </p>  
	</tr>
	</tbody>
	</table>
	<table border="0">
	<tbody><tr>
		<td width="140"><a href="https://erfect2020.github.io/"><img src="imgs/DeFusion.jpg" border="1"width="210"></a></td>
		<td width="20"></td>
		<td valign="middle" width="900"><p class="content"><strong>Fusion from Decomposition: A Self-Supervised Decomposition Approach for Image Fusion.</strong></p>
		<p class="content"><a href="http://erfect2020.github.io/"><b>Pengwei Liang</b></a>, <a href="http://homepage.hit.edu.cn/jiangjunjun">Junjun Jiang*</a>,
			<a href="http://homepage.hit.edu.cn/xmliu">Xianming Liu</a>,
			<a href="https://sites.google.com/site/jiayima2013/jiayi-ma-%E9%A9%AC%E4%BD%B3%E4%B9%89-professor">Jiayi Ma</a>
		</p>
		<p class="content">European Conference on Computer Vision (ECCV), 2022</p>
			<p class="margin-small">&nbsp;</p>
		<p class="content">
		<strong><a href="https://github.com/erfect2020/DecompositionForFusion">Project</a></strong> |
				<strong> <a href="https://github.com/erfect2020/DecompositionForFusion">Code</a></strong> |
				<strong><a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136780706.pdf">Paper</a></strong> |
				<strong><a href="https://github.com/hitcszx/lnl_sr#Citations">BibTex</a></strong> <br> </p>  </tr>
	</tbody>
	</table>
	<table border="0">
	<tbody><tr>
		<td width="140"><a href="https://erfect2020.github.io/"><img src="imgs/PrecepLoss.jpg" border="1"width="210"></a></td>
		<td width="20"></td>
		<td valign="middle" width="900"><p class="content"><strong>Image Deblurring by Exploring In-depth Properties of Transformer.</strong></p>
		<p class="content"><a href="http://erfect2020.github.io/"><b>Pengwei Liang</b></a>, <a href="http://homepage.hit.edu.cn/jiangjunjun">Junjun Jiang*</a>,
			<a href="http://homepage.hit.edu.cn/xmliu">Xianming Liu</a>,
			<a href="https://sites.google.com/site/jiayima2013/jiayi-ma-%E9%A9%AC%E4%BD%B3%E4%B9%89-professor">Jiayi Ma</a>
		</p>
		<p class="content">IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2024</p>
			<p class="margin-small">&nbsp;</p>
		<p class="content">
		<strong><a href="https://github.com/erfect2020/TransformerPerceptualLoss">Project</a></strong> |
				<strong> <a href="https://github.com/erfect2020/TransformerPerceptualLoss">Code</a></strong> |
				<strong><a href="https://ieeexplore.ieee.org/document/10443067/">Paper</a></strong> |
				<strong><a href="https://github.com/erfect2020/TransformerPerceptualLoss#Basic usage">BibTex</a></strong> <br> </p>  </tr>
	</tbody>
	</table>
	<table border="0">
	<tbody><tr>
		<td width="140"><a href="https://erfect2020.github.io/"><img src="imgs/ViTDeblur.jpg" border="1"width="210"></a></td>
		<td width="20"></td>
		<td valign="middle" width="900"><p class="content"><strong>Decoupling Image Deblurring into Twofold: A Hierarchical Model for Defocus Deblurring.</strong></p>
		<p class="content"><a href="http://erfect2020.github.io/"><b>Pengwei Liang</b></a>, <a href="http://homepage.hit.edu.cn/jiangjunjun">Junjun Jiang*</a>,
			<a href="http://homepage.hit.edu.cn/xmliu">Xianming Liu</a>,
			<a href="https://sites.google.com/site/jiayima2013/jiayi-ma-%E9%A9%AC%E4%BD%B3%E4%B9%89-professor">Jiayi Ma</a>
		</p>
		<p class="content">IEEE Transactions on Computational Imaging (TCI), 2024</p>
			<p class="margin-small">&nbsp;</p>
		<p class="content">
		<strong><a href="https://github.com/erfect2020/ContextualDeblur">Project</a></strong> |
				<strong> <a href="https://github.com/erfect2020/ContextualDeblur">Code</a></strong> |
				<strong><a href="https://ieeexplore.ieee.org/document/10637737">Paper</a></strong> |
				<strong><a href="https://github.com/erfect2020/ContextualDeblur#bib">BibTex</a></strong> <br> </p>  </tr>
	</tbody>
	</table>
	<table border="0">
	<tbody><tr>
		<td width="140"><a href="https://erfect2020.github.io/"><img src="imgs/FusionINV.jpg" border="1"width="210"></a></td>
		<td width="20"></td>
		<td valign="middle" width="900"><p class="content"><strong>FusionINV: A DiffusionBased Approach for Multi-Modal Image Fusion.</strong></p>
			<p class="content"> <a href="http://erfect2020.github.io/"><b>Pengwei Liang</b></a>, <a href="http://homepage.hit.edu.cn/jiangjunjun">Junjun Jiang*</a>,
				<a href="https://scholar.google.com/citations?user=x6QQGQkAAAAJ">Qing Ma</a>, <a href="https://chenyang4.github.io/">Chenyang Wang</a>, 
				<a href="http://homepage.hit.edu.cn/xmliu">Xianming Liu</a>,
				<a href="https://sites.google.com/site/jiayima2013/jiayi-ma-%E9%A9%AC%E4%BD%B3%E4%B9%89-professor">Jiayi Ma</a>
		</p>
		<p class="content">IEEE Transactions on Image Processing (TIP), 2025.</p>
			<p class="margin-small">&nbsp;</p>
		<p class="content">
		<strong><a href="https://github.com/erfect2020/FusionINV">Project</a></strong> |
				<strong> <a href="https://github.com/erfect2020/FusionINV">Code</a></strong> |
				<strong><a href="https://ieeexplore.ieee.org/document/11114795">Paper</a></strong> |
				<strong><a href="https://github.com/erfect2020/FusionINV#-citation">BibTex</a></strong> <br> </p>  </tr>
	</tbody>
	</table>
	<table border="0">
	<tbody><tr>
		<td width="140"><a href="https://erfect2020.github.io/"><img src="imgs/Pixel2Pixel.jpg" border="1"width="210"></a></td>
		<td width="20"></td>
		<td valign="middle" width="900"><p class="content"><strong>Pixel2Pixel: A Pixelwise Approach for Zero-Shot Single Image Denoising.</strong></p>
		<p class="content"><a href="https://scholar.google.com/citations?user=x6QQGQkAAAAJ">Qing Ma</a>, 
			<a href="http://homepage.hit.edu.cn/jiangjunjun">Junjun Jiang*</a>,
			<a href="https://hitcszx.github.io/">Xiong Zhou</a>,
			<a href="http://erfect2020.github.io/"><b>Pengwei Liang</b></a>,
			<a href="http://homepage.hit.edu.cn/xmliu">Xianming Liu</a>, 
			<a href="https://sites.google.com/site/jiayima2013/jiayi-ma-%E9%A9%AC%E4%BD%B3%E4%B9%89-professor">Jiayi Ma</a>
		</p>
		<p class="content">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2025.</p>
			<p class="margin-small">&nbsp;</p>
		<p class="content">
		<strong><a href="https://github.com/qingma2016/Pixel2Pixel">Project</a></strong> |
				<strong> <a href="https://github.com/qingma2016/Pixel2Pixel">Code</a></strong> |
				<strong><a href="https://ieeexplore.ieee.org/document/10908805/">Paper</a></strong> |
				<strong><a href="bib/Pixel2Pixel.bib" download>BibTeX</a></strong> <br> </p>  </tr>
	</tbody>
	</table>
	<table border="0">
	<tbody><tr>
		<td width="140"><a href="https://erfect2020.github.io/"><img src="imgs/MAEFuse.jpg" border="1"width="210"></a></td>
		<td width="20"></td>
		<td valign="middle" width="900"><p class="content"><strong>MaeFuse: Transferring omni features with pretrained masked autoencoders for infrared and visible image fusion via guided training.</strong></p>
		<p class="content">
			<a href="https://villa.jianzhang.tech/people/jiayang-li-%E6%9D%8E%E4%BD%B3%E9%98%B3/">Jiayang Li</a>,
			<a href="http://homepage.hit.edu.cn/jiangjunjun">Junjun Jiang*</a>,
			<a href="http://erfect2020.github.io/"><b>Pengwei Liang</b></a>,
			<a href="https://sites.google.com/site/jiayima2013/jiayi-ma-%E9%A9%AC%E4%BD%B3%E4%B9%89-professor">Jiayi Ma</a>,
			<a href="https://liqiangnie.github.io/index.html">Liqiang Nie</a>
		</p>
		<p class="content">IEEE Transactions on Image Processing (TIP), 2025.</p>
			<p class="margin-small">&nbsp;</p>
		<p class="content">
		<strong><a href="https://github.com/Henry-Lee-real/MaeFuse">Project</a></strong> |
				<strong> <a href="https://github.com/Henry-Lee-real/MaeFuse">Code</a></strong> |
				<strong><a href="https://ieeexplore.ieee.org/document/10893688/">Paper</a></strong> |
				<strong><a href="https://github.com/Henry-Lee-real/MaeFuse#citation">BibTex</a></strong> <br> </p>  </tr>
	</tbody>
	</table>
	<table border="0">
	<tbody><tr>
		<td width="140"><a href="https://erfect2020.github.io/"><img src="imgs/TTAD.jpg" border="1"width="210"></a></td>
		<td width="20"></td>
		<td valign="middle" width="900"><p class="content"><strong>Robust Test-Time Adaptation for Single Image Denoising Using Deep Gaussian Prior.</strong></p>
		<p class="content">
			<a href="https://scholar.google.com/citations?user=x6QQGQkAAAAJ">Qing Ma</a>,
			<a href="http://erfect2020.github.io/"><b>Pengwei Liang</b></a>,
			<a href="https://hitcszx.github.io/"><b>Xiong Zhou</b></a>, 
			<a href="https://sites.google.com/site/jiayima2013/jiayi-ma-%E9%A9%AC%E4%BD%B3%E4%B9%89-professor">Jiayi Ma</a>,
			<a href="http://homepage.hit.edu.cn/jiangjunjun">Junjun Jiang*</a>,
			<a href="https://www.polyu.edu.hk/ise/people/academic-staff/zhe-peng/">Zhe Peng*</a>
		</p>
		<p class="content">International Conference on Computer Vision (ICCV) 2025</p>
			<p class="margin-small">&nbsp;</p>
		<p class="content">
		<strong><a href="https://github.com/qingma2016/TTAD">Project</a></strong> |
				<strong> <a href="https://github.com/qingma2016/TTAD">Code</a></strong> |
				<strong><a href="https://openaccess.thecvf.com/content/ICCV2025/html/Ma_Robust_Test-Time_Adaptation_for_Single_Image_Denoising_Using_Deep_Gaussian_ICCV_2025_paper.html">Paper</a></strong> |
				<strong><a href="bib/TTAD.bib" download>BibTeX</a></strong> <br> </p>  </tr>
	</tbody>
	</table>
	<!-- <table border="0">
	<tbody><tr>
		<td width="140"><a href="https://hitcszx.github.io/"><img src="imgs/nerf.jpg" border="1"width="210"></a></td>
		<td width="20"></td>
		<td valign="middle" width="900"><p class="content"><strong>Neural Field Classifiers via Target Encoding and Classification Loss</strong></p>
		<p class="content"> <b>Xindi Yang</b>, Zeke Xie, <a href="https://hitcszx.github.io/"><b>Xiong Zhou</b></a>, Boyu Liu, Buhua Liu, Yi Liu, Haoran Wang, YUNFENG CAI, Mingming Sun
		</p>
		<p class="content">International Conference on Learning Representations (ICLR) 2024</p>
			<p class="margin-small">&nbsp;</p>
		<p class="content">
		<strong><a href="https://openreview.net/forum?id=9NqC72m31m">Project</a></strong> |
				<strong> <a href="https://openreview.net/forum?id=9NqC72m31m">Code</a></strong> |
				<strong><a href="https://openreview.net/pdf?id=9NqC72m31m">Paper</a></strong> |
				<strong><a href="https://openreview.net/forum?id=9NqC72m31m">BibTex</a></strong> <br> </p>  </tr>
	</tbody>
	</table> -->
	<div style="display:none">
	
	<script type="text/javascript" src="http://gostats.com/js/counter.js"></script>
	<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
	_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script>
	
	</div>
	<br>
	<br>
	<p id="service" class="title-large">Service</p>
	<ul>
		<li><p class="content">Reviewer : TPAMI, TIP, TNNLS, PR, Information Fusion, IEEE JAS, ESWA</p></li>
		<!-- <li><p class="content">Outstanding Reviewer at ICML 2022 (top 10%)</p></li> -->
	</ul>
	<br>
	<br>
	<p id="sect-awards" class="title-large">Awards</p>
	<ul>
		<li><p class="content">Tencent Scholarship, 2024</p></li>
		<li><p class="content">Academic Innovation Award (Wuhan University), 2020</p></li>
		<li><p class="content">The 9th (Rongwei New Energy) National University Student Social Practice and Science Contest on Energy Saving & Emission Reduction, 2016</p></li>
	</ul>
	<div align="center" style="margin:auto;padding-top:10px">
            <div style="width:15%">
                <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=h87tkcVTteUC8QVeZ_wn1pK4-aVDcUQQqwJEgnZyQBI"></script>
            </div>
        </div>
	</body></html>
